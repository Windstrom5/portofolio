<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <title>Sakura VN - Manga Style</title>

  <script type="importmap">
      {
        "imports": {
          "three": "https://unpkg.com/three@0.152.2/build/three.module.js",
          "three/examples/jsm/loaders/GLTFLoader.js": "https://unpkg.com/three@0.152.2/examples/jsm/loaders/GLTFLoader.js"
        }
      }
    </script>

  <style>
    body {
      margin: 0;
      overflow: hidden;
      background-color: transparent;
      font-family: "Segoe UI", sans-serif;
    }

    /* === MANGA BUBBLE FIX === */
    #bubble-container {
      position: absolute;
      bottom: 20px;
      /* Positioned at bottom for VN style */
      left: 50%;
      transform: translateX(-50%);
      z-index: 20;
      width: 85%;
      max-width: 350px;
      opacity: 0;
      transition: opacity 0.5s ease-in-out;
    }

    #speech-bubble {
      background: white;
      border: 4px solid black;
      border-radius: 20px;
      padding: 15px;
      color: black;
      font-weight: 800;
      position: relative;
      box-shadow: 6px 6px 0px rgba(0, 0, 0, 0.2);
      font-family: "Comic Sans MS", "Chalkboard SE", sans-serif;
      font-size: 0.95rem;
      line-height: 1.3;
      text-align: center;

      /* FIX: Even shorter max-height and scrolling */
      max-height: 160px;
      overflow-y: auto;
      display: block;
    }

    /* Hide scrollbar but keep functionality */
    #speech-bubble::-webkit-scrollbar {
      width: 0px;
    }

    /* === THE "ARROW" (TAIL) FIX === */
    #speech-bubble::after {
      content: "";
      position: absolute;
      top: -18px;
      /* Flipped to top for pointing up */
      left: 50%;
      transform: translateX(-50%) rotate(180deg);
      /* Rotate to point up */
      border-width: 18px 15px 0;
      border-style: solid;
      border-color: white transparent;
      z-index: 2;
    }

    #speech-bubble::before {
      content: "";
      position: absolute;
      top: -25px;
      /* Flipped to top */
      left: 50%;
      transform: translateX(-50%) rotate(180deg);
      /* Rotate to point up */
      border-width: 22px 18px 0;
      border-style: solid;
      border-color: black transparent;
      z-index: 1;
    }
  </style>
</head>

<body>
  <div id="loading-overlay" style="
        position: fixed;
        inset: 0;
        background: rgba(0, 0, 0, 0.7);
        display: flex;
        align-items: center;
        justify-content: center;
        color: white;
        font-size: 1.4rem;
        z-index: 9999;
        transition: opacity 0.6s;
      ">
    <div style="text-align: center">
      <div class="spinner" style="
            border: 6px solid #f3f3f3;
            border-top: 6px solid #3498db;
            border-radius: 50%;
            width: 50px;
            height: 50px;
            animation: spin 1.2s linear infinite;
            margin: 0 auto 20px;
          "></div>
      <div>Loading Sakura (voice engine)...</div>
      <div id="status-text" style="font-size: 0.9rem; margin-top: 12px; opacity: 0.8">
        Japanese voice may take a few seconds...
      </div>
    </div>
  </div>

  <!-- VOICE UNLOCK OVERLAY -->
  <div id="voice-unlock" style="
        position: fixed;
        inset: 0;
        background: rgba(0, 0, 0, 0.4);
        display: none;
        align-items: center;
        justify-content: center;
        color: white;
        z-index: 9998;
        cursor: pointer;
        backdrop-filter: blur(2px);
      ">
    <div style="text-align: center; background: rgba(0,0,0,0.8); padding: 20px; border-radius: 15px; border: 2px solid pink;">
      <div style="font-size: 3rem; margin-bottom: 10px;">ðŸ”Š</div>
      <div style="font-weight: bold;">Tap to Enable Sakura's Voice</div>
      <div style="font-size: 0.8rem; margin-top: 10px; opacity: 0.7;">(Required by browser security)</div>
    </div>
  </div>
  <div id="bubble-container">
    <div id="speech-bubble">
      <div id="voice-status" style="position: absolute; top: 10px; right: 10px; font-size: 0.6rem; color: #ff6b9d; font-weight: bold; opacity: 0.8;"></div>
      <span id="text-display">...</span>
    </div>
  </div>

  <script type="module">
    import * as THREE from "three";
    import { GLTFLoader } from "three/examples/jsm/loaders/GLTFLoader.js";
    import { VRMLoaderPlugin } from "https://unpkg.com/@pixiv/three-vrm@2.0.0/lib/three-vrm.module.js";

    /* ===================== 3D SETUP ===================== */
    let scene, camera, renderer, vrm;
    let clock = new THREE.Clock();
    let isSleeping = false;
    let voicesReady = false;
    let hasJapaneseVoice = false;
    window.speechSynthesis.onvoiceschanged = () => {
      checkVoices();
    };
    setTimeout(checkVoices, 300);
    setTimeout(checkVoices, 1200);
    scene = new THREE.Scene();
    camera = new THREE.PerspectiveCamera(
      30,
      window.innerWidth / window.innerHeight,
      0.1,
      100,
    );
    camera.position.set(0.0, 1.4, 2.8);

    renderer = new THREE.WebGLRenderer({ alpha: true, antialias: true });
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.setPixelRatio(window.devicePixelRatio);
    document.body.appendChild(renderer.domElement);

    scene.add(new THREE.AmbientLight(0xffffff, 0.8));
    const sun = new THREE.DirectionalLight(0xffffff, 0.6);
    sun.position.set(1, 1, 1).normalize();
    scene.add(sun);

    const loader = new GLTFLoader();
    loader.register((parser) => new VRMLoaderPlugin(parser));
    loader.load("/models/Aiko.vrm", (gltf) => {
      vrm = gltf.userData.vrm;
      scene.add(vrm.scene);

      // Initial Pose
      const rb = (n, v) => {
        const b = vrm.humanoid.getNormalizedBoneNode(n);
        if (b) b.rotation.z = v;
      };
      rb("leftUpperArm", -1.3);
      rb("rightUpperArm", 1.3);
      rb("leftLowerArm", 0.3);
      rb("rightLowerArm", -0.3);

      const neck = vrm.humanoid.getNormalizedBoneNode("neck");
      if (neck) neck.rotation.x = 0; // Awake

      // === CRITICAL: Notify parent (Flutter) that VRM is ready ===
      console.log("VRM: Model loaded, sending vrm_ready to parent");
      
      // Setup LookAt - We use manual bone rotation now for better control
      // vrm.lookAt.target = lookAtTarget; 
      // vrm.lookAt.autoUpdate = true;
      
      if (window.parent && window.parent !== window) {
        window.parent.postMessage({ type: 'vrm_ready' }, '*');
      }
    });

    /* ===================== VN STYLE TEXT & VOICE ===================== */
    let typingInterval;
    let hideTimeout;
    let selectedVoice = null;

    function checkVoices() {
      const voices = window.speechSynthesis.getVoices();
      if (voices.length > 0 && !voicesReady) {
        voicesReady = true;
        const jaVoices = voices.filter((v) => v.lang.startsWith("ja"));
        hasJapaneseVoice = jaVoices.length > 0;

        console.log(
          "Voices loaded:",
          voices.length,
          "Japanese voices:",
          jaVoices.length,
        );
        console.log(
          "Available ja voices:",
          jaVoices.map((v) => `${v.name} (${v.lang})`),
        );

        // Pre-select the best voice
        selectedVoice = selectBestVoice(voices);
        console.log("Selected voice:", selectedVoice?.name || "default");

        // Hide loading when voices exist
        hideLoading();
      }
    }

    // Priority voice selection for more natural sound
    function selectBestVoice(voices) {
      const jaVoices = voices.filter((v) => v.lang === "ja-JP" || v.lang.startsWith("ja"));

      // Priority order for natural-sounding female voices
      const priorityPatterns = [
        /Microsoft.*Nanami/i,      // Edge Neural voice (High quality female)
        /Microsoft.*Haruka/i,      // Microsoft Female
        /Google.*æ—¥æœ¬èªž/i,          // Google Japanese
        /Google.*Japanese/i,       // Google Japanese alt
        /Neural.*Female/i,         // Neural female
        /Natural.*Female/i,        // Natural female
        /Kyoko/i,                  // Apple Kyoko (Female)
        /O-Ren/i,                  // Apple O-Ren (Female)
        /Mei-Mei/i,                // Possible other female names
        /Airi/i,
      ];

      for (const pattern of priorityPatterns) {
        const match = jaVoices.find((v) => pattern.test(v.name));
        if (match) return match;
      }

      // Fallback to any Japanese voice
      return jaVoices[0] || voices[0];
    }

    function hideLoading() {
      const overlay = document.getElementById("loading-overlay");
      if (overlay) {
        overlay.style.opacity = "0";
        setTimeout(() => {
          overlay.style.display = "none";
        }, 600);
      }
    }

    // Main speak function - now accepts both Japanese (for voice) and English (for display)
    let audioUnlocked = false;
    
    // Auto-unlock audio: try immediately and on first message from parent
    function tryUnlockAudio() {
      if (audioUnlocked) return;
      try {
        const u = new SpeechSynthesisUtterance("");
        u.volume = 0;
        window.speechSynthesis.speak(u);
        audioUnlocked = true;
        console.log("VRM: Audio unlocked automatically");
      } catch(e) {
        console.warn("VRM: Auto audio unlock failed, waiting for interaction", e);
      }
    }

    // Try immediately
    tryUnlockAudio();

    // Also unlock on click (fallback for strict browsers)
    window.addEventListener('click', () => {
      tryUnlockAudio();
      document.getElementById("voice-unlock").style.display = "none";
    });
    
    document.getElementById("voice-unlock").addEventListener('click', () => {
      tryUnlockAudio();
      document.getElementById("voice-unlock").style.display = "none";
    });

    // Also unlock on first postMessage from parent (counts as user gesture in many browsers)
    window.addEventListener('message', function unlockOnMessage() {
      tryUnlockAudio();
      window.removeEventListener('message', unlockOnMessage);
    });

    let currentAudio = null;
    let audioContext = null;
    let analyser = null;
    let dataArray = null;

    async function speakWithVoicevox(text, speakerId = 46) {
      const statusEl = document.getElementById("voice-status");
      try {
        if (statusEl) statusEl.textContent = "âŒ› SYNCING...";
        
        // Use api.tts.quest v3 for better stability
        const url = `https://api.tts.quest/v3/voicevox/synthesis?text=${encodeURIComponent(text)}&speaker=${speakerId}`;
        
        const response = await fetch(url);
        if (!response.ok) throw new Error("Voicevox API request failed");
        
        const data = await response.json();
        if (!data.success) throw new Error("Voicevox synthesis failed");

        const statusUrl = data.audioStatusUrl;
        const audioUrl = data.mp3DownloadUrl;

        // Poll for readiness
        let isReady = false;
        let attempts = 0;
        const maxAttempts = 15; // ~7.5 seconds

        if (statusEl) statusEl.textContent = "âœ¦ GENERATING...";

        while (!isReady && attempts < maxAttempts) {
          const statusRes = await fetch(statusUrl);
          const statusData = await statusRes.json();
          if (statusData.isAudioReady) {
            isReady = true;
          } else {
            attempts++;
            await new Promise(r => setTimeout(r, 500));
          }
        }

        if (!isReady) throw new Error("Voicevox timeout");
        if (statusEl) statusEl.textContent = "âœ¦ NEURAL";
        
        if (currentAudio) {
          currentAudio.pause();
          window.speechSynthesis.cancel();
        }
        
        currentAudio = new Audio(audioUrl);
        
        // Setup Analysis
        if (!audioContext) {
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
        }
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
        }

        const source = audioContext.createMediaElementSource(currentAudio);
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        source.connect(analyser);
        analyser.connect(audioContext.destination);
        dataArray = new Uint8Array(analyser.frequencyBinCount);

        currentAudio.onplay = () => {
          if (window.parent && window.parent !== window) {
            window.parent.postMessage({ type: 'speechStart' }, '*');
          }
        };

        currentAudio.onended = () => {
          if (window.parent && window.parent !== window) {
            window.parent.postMessage({ type: 'speechEnd' }, '*');
          }
          hideTimeout = setTimeout(() => {
            document.getElementById("bubble-container").style.opacity = "0";
          }, 5000);
        };

        await currentAudio.play();
        return true;
      } catch (e) {
        console.warn("VRM: Voicevox failed, falling back to browser TTS", e);
        if (statusEl) statusEl.textContent = "âœ¦ LEGACY";
        return false;
      }
    }

    async function speak(textOrData) {
      let japaneseText, englishText;

      if (typeof textOrData === 'object' && textOrData !== null) {
        japaneseText = textOrData.japanese || textOrData.jp || textOrData.text || "";
        englishText = textOrData.english || textOrData.en || japaneseText;
      } else {
        japaneseText = textOrData || "";
        englishText = japaneseText;
      }

      if (!japaneseText) return;

      const cleanJapanese = japaneseText.replace(/\*/g, "").replace(/#/g, "");
      const cleanEnglish = englishText.replace(/\*/g, "").replace(/#/g, "");

      const container = document.getElementById("bubble-container");
      const display = document.getElementById("text-display");

      if (typingInterval) clearInterval(typingInterval);
      if (hideTimeout) clearTimeout(hideTimeout);

      display.innerHTML = "";
      container.style.opacity = "1";

      // 1. Try Voicevox first (Natural)
      const voicevoxSuccess = await speakWithVoicevox(cleanJapanese);
      
      if (voicevoxSuccess) {
          // Voicevox is playing, just start typewriter
          startSyncedTypewriter(cleanEnglish, { text: cleanJapanese, rate: 1.0 });
          return;
      }

      // 2. Fallback to Browser TTS (Robotic)
      window.speechSynthesis.cancel();
      
      if (!voicesReady) {
        checkVoices();
        if (!voicesReady) {
          display.innerHTML = "Voice system loading... â™¡<br>" + cleanEnglish;
          document.getElementById("voice-unlock").style.display = "flex";
          return;
        }
      }

      if (!audioUnlocked) {
         document.getElementById("voice-unlock").style.display = "flex";
      }

      const utter = new SpeechSynthesisUtterance(cleanJapanese);
      utter.lang = "ja-JP";
      if (selectedVoice) utter.voice = selectedVoice;
      utter.pitch = 1.45;
      utter.rate = 1.05;

      utter.onstart = () => {
        startSyncedTypewriter(cleanEnglish, utter);
        if (window.parent && window.parent !== window) {
          window.parent.postMessage({ type: 'speechStart' }, '*');
        }
      };

      utter.onend = () => {
        display.innerHTML = cleanEnglish;
        document.getElementById("speech-bubble").scrollTop = document.getElementById("speech-bubble").scrollHeight;
        if (window.parent && window.parent !== window) {
          window.parent.postMessage({ type: 'speechEnd' }, '*');
        }
        hideTimeout = setTimeout(() => {
          container.style.opacity = "0";
        }, 5000);
      };

      window.speechSynthesis.speak(utter);
    }

    // Synced typewriter that matches speech duration
    function startSyncedTypewriter(text, utterance) {
      const container = document.getElementById("bubble-container");
      const display = document.getElementById("text-display");

      if (typingInterval) clearInterval(typingInterval);

      // Estimate speech duration based on Japanese text length and rate
      // Average Japanese speech: ~7-8 characters per second
      const estimatedDuration = (utterance.text.length / 7) * 1000 / utterance.rate;
      const intervalMs = Math.max(30, estimatedDuration / text.length);

      let i = 0;
      display.innerHTML = "";

      typingInterval = setInterval(() => {
        if (i < text.length) {
          display.innerHTML += text.charAt(i);
          i++;
          document.getElementById("speech-bubble").scrollTop =
            document.getElementById("speech-bubble").scrollHeight;
        } else {
          clearInterval(typingInterval);
        }
      }, intervalMs);
    }

    // --- Message Handling ---
    const urlParams = new URLSearchParams(window.location.search);
    const myViewId = urlParams.get('viewId');
    console.log(`VRM: Instance started with viewId: ${myViewId}`);

    window.addEventListener("message", (event) => {
      const data = event.data;
      if (!data) return;

      // Filter by targetId if provided and we have our own viewId
      if (myViewId && data.targetId && data.targetId !== myViewId) {
        return; 
      }

      const type = data.type;
      
      if (type === "speak") {
        if (voicesReady) {
          // Handle bilingual format: {japanese, english} or legacy {text}
          if (data.japanese && data.english) {
            speak({
              japanese: data.japanese,
              english: data.english
            });
          } else {
            // Legacy format or fallback
            speak(data.text || data.japanese || data.english || "");
          }
        } else {
          document.getElementById("text-display").innerHTML =
            "Still loading Sakura's voice... â™¡<br>" + (data.english || data.text || "");
        }
      } else if (type === "emotion") {
        // Handle emotion changes: joy, angry, sorrow, fun, lookup, lookdown, etc.
        const emotion = data.emotion;
        if (vrm && emotion) {
           // Reset common expressions
           vrm.expressionManager.setValue("joy", 0);
           vrm.expressionManager.setValue("angry", 0);
           vrm.expressionManager.setValue("sorrow", 0);
           vrm.expressionManager.setValue("fun", 0);
           
           // Set new emotion
           if (["joy", "angry", "sorrow", "fun"].includes(emotion)) {
             vrm.expressionManager.setValue(emotion, 1.0);
           }
        }
      } else if (type === "wake") {
        isSleeping = false;
        if (vrm) {
          const neck = vrm.humanoid.getNormalizedBoneNode("neck");
          if (neck) neck.rotation.x = 0;
        }
        speak({
          japanese: "ã‚ã‚‰ï¼ŸãŠå®¢æ§˜ï¼Ÿã‚„ã£ã¨æ¥ãŸã®ã­ï¼å¾…ã£ã¦ãŸã‚ˆï¼",
          english: "H-Huh? A visitor? You're finally here! I've been waiting! â™¡"
        });
      } else if (type === "control") {
        // Head tracking data
        if (data.x !== undefined && data.y !== undefined) {
           targetX = data.x; // -1 to 1
           targetY = data.y; // -1 to 1
        }
      }
    });

    /* ===================== LOOK AT TARGET ===================== */
    // We will use manual bone rotation for more natural "whole head" movement
    let targetX = 0;
    let targetY = 0;
    let currentX = 0;
    let currentY = 0;

    /* ===================== ANIMATION LOOP ===================== */
    function animate() {
      requestAnimationFrame(animate);
      const delta = clock.getDelta();
      
      if (vrm) {
        const t = clock.elapsedTime;

        // Smoothly interpolate current look direction
        const damping = 4.0 * delta;
        currentX += (targetX - currentX) * damping;
        currentY += (targetY - currentY) * damping;

        // --- NECK & HEAD ROTATION ---
        const neck = vrm.humanoid.getNormalizedBoneNode("neck");
        const head = vrm.humanoid.getNormalizedBoneNode("head");
        const spine = vrm.humanoid.getNormalizedBoneNode("spine");
        
        if (neck && head && !isSleeping) {
           // Rotate neck (Yaw and Pitch)
           // Limits: Y (Left/Right) +/- 0.7 rad, X (Up/Down) +/- 0.5 rad
           neck.rotation.y = currentX * 0.6; 
           neck.rotation.x = currentY * 0.5;
           
           // Rotate head slightly less for natural curve
           head.rotation.y = currentX * 0.3;
           head.rotation.x = currentY * 0.3;
           
           // Rotate spine slightly for body follow
           if (spine) {
             spine.rotation.y = currentX * 0.1;
             spine.rotation.x = currentY * 0.1;
           }
        }

        if (isSleeping) {
          vrm.expressionManager.setValue("blink", 1.0);
          vrm.expressionManager.setValue("aa", 0);
          const sleepBreath = Math.sin(t * 1.5) * 0.1;
          if(spine) spine.rotation.x = sleepBreath;
        } else {
          vrm.expressionManager.setValue(
            "blink",
            Math.sin(t * 0.5) > 0.98 ? 1 : 0,
          );

          // --- MOUTH MAPPING ---
          let mouthOpen = 0;
          if (currentAudio && !currentAudio.paused) {
            // Audio-reactive mouth
            if (analyser && dataArray) {
              analyser.getByteFrequencyData(dataArray);
              let sum = 0;
              for (let i = 0; i < dataArray.length; i++) {
                sum += dataArray[i];
              }
              const average = sum / dataArray.length;
              mouthOpen = Math.min(1.0, average / 40); // Sensitive enough for speech
            }
          } else if (window.speechSynthesis.speaking) {
            // Fallback: Sine wave for browser TTS
            mouthOpen = Math.abs(Math.sin(t * 15)) * 0.6;
          }

          vrm.expressionManager.setValue("aa", mouthOpen);

          const breath = Math.sin(t * 1) * 0.05;
          vrm.humanoid.getNormalizedBoneNode("spine").rotation.x = breath;
        }

        vrm.update(delta);
      }
      renderer.render(scene, camera);
    }

    animate();

    window.addEventListener("resize", () => {
      camera.aspect = window.innerWidth / window.innerHeight;
      camera.updateProjectionMatrix();
      renderer.setSize(window.innerWidth, window.innerHeight);
    });
    setTimeout(hideLoading, 8000);
  </script>
</body>

</html>